[TOC]

# ã€ŠGenerative Adversarial Netsã€‹

[è®ºæ–‡é“¾æ¥](https://arxiv.org/abs/1406.2661)
[è§†é¢‘é“¾æ¥](https://youtu.be/dyZXMqnS_x0)
è¿™ç¯‡è®ºæ–‡æå‡ºäº†GAN

-------

## ç”Ÿæˆæ¨¡å‹

### Need Explicit Density
Pixel RNN/CNN
VAE
Markov Chain
Boltzmann Machine

### No Need Explicit Density
GAN
GSN

-------

## Intuitional Explanation

![](./res/intuitional interpretation.jpg)

-------

## Objective Function

### æå¤§ä¼¼ç„¶ä¼°è®¡

å¯¹äºçœŸå®æ•°æ®$x$å’Œç”Ÿæˆæ•°æ®$G(z)$ï¼Œç»è¿‡åˆ¤åˆ«å™¨åˆ¤åˆ«åçš„ï¼Œ$D$è®¤ä¸º$x$æ˜¯çœŸæ ·æœ¬çš„æ¦‚ç‡ä¸º$D(x)$ï¼Œ$D$è®¤ä¸º$G(z)$æ˜¯å‡æ ·æœ¬çš„æ¦‚ç‡ä¸º$1-D(G(z))$ï¼Œé‚£ä¹ˆå¯¹äº$D$æœ‰$log$ä¼¼ç„¶å‡½æ•°ä¸ºï¼š
$$L=log[D(x)*(1-D(G(z)))]$$
[æå¤§ä¼¼ç„¶ä¼°è®¡-å¼ å®‡](https://zhuanlan.zhihu.com/p/50484278)
[æå¤§ä¼¼ç„¶ä¼°è®¡-æå®æ¯…](https://youtu.be/CppM-5Vogl4)

### GANçš„ç›®æ ‡å‡½æ•°

$$ \min _{G}\max _{ D } V(D,G)={ E }_{ x ï½ { p }_  { data } (x) }[logD(x)] + { E }_{ z ï½ { p }_{ z }(z) }[log(1-D(G(z)))]\ $$
$D(x)$å’Œ$D(G(z))$åˆ†åˆ«è¡¨ç¤º$x$å’Œ$G(z)$ç»è¿‡åˆ¤åˆ«å™¨$D$çš„åˆ¤åˆ«åï¼Œ$D$è®¤ä¸ºè¾“å…¥æ ·æœ¬æ˜¯çœŸæ ·æœ¬çš„æ¦‚ç‡ï¼Œåˆ™$1-D(G(z))$è¡¨ç¤º$D$å°†å‡æ ·æœ¬åˆ¤æ–­ä¸ºå‡çš„æ¦‚ç‡ï¼›é‚£ä¹ˆï¼ŒçœŸå®çš„æ¦‚ç‡åˆ†å¸ƒä¸$D$åˆ¤æ–­å‡ºæ¥çš„æƒ…å†µåˆ—è¡¨å¦‚ä¸‹ï¼š

**Note:$D$è¾“å‡ºçš„æ˜¯æ¦‚ç‡ï¼Œé‚£ä¹ˆ$D$çš„è¾“å‡ºå±‚çš„æ¿€æ´»å‡½æ•°å¿…é¡»æ˜¯$sigmoid$**

| $D$ | $D$å°†çœŸæ ·æœ¬$x$åˆ¤æ–­ä¸ºçœŸçš„æ¦‚ç‡:$D(x) $| $D$å°†å‡æ ·æœ¬$G(z)$åˆ¤æ–­ä¸ºå‡çš„æ¦‚ç‡:$1-D(G(z)$ |
| --- | --- | --- |
| çœŸå®æƒ…å†µ | çœŸæ ·æœ¬$x$ä¸ºçœŸçš„æ¦‚ç‡:1 | å‡æ ·æœ¬$G(z)$ä¸ºå‡çš„æ¦‚ç‡:1 |
| ç”¨äº¤å‰ç†µä½œä¸ºç›®æ ‡å‡½æ•° | $1*log[D(x)]å¯¹åº”ç¬¬ä¸€é¡¹$ | $1*log[1-D(G(z))]$å¯¹åº”ç¬¬äºŒé¡¹ |

### å¯¹æŠ—

$D$çš„ç›®æ ‡æ˜¯è¦å°½å¯èƒ½æŠŠçœŸçš„æ ·æœ¬åˆ¤æ–­ä¸ºçœŸï¼Œå¯¹åº”æœ€å¤§åŒ–ç¬¬ä¸€é¡¹ï¼š${ E }_{ x ï½ { p }_  { data } (x) }[logD(x)]$
æŠŠå‡çš„æ ·æœ¬åˆ¤æ–­ä¸ºå‡ï¼Œå¯¹åº”æœ€å¤§åŒ–ç¬¬äºŒé¡¹ï¼š${ E }_{ z ï½ { p }_{ z }(z) }[log(1-D(G(z)))]\ $
æ€»ä¹‹ï¼Œä¹Ÿå°±æ˜¯è¯´$D$è¦æœ€å¤§åŒ–Objective Functionï¼›

åŒç†ï¼Œ$G$çš„ç›®æ ‡æ˜¯è¦å°½å¯èƒ½çš„è®©$D$å°†è‡ªå·±ç”Ÿæˆçš„å‡æ ·æœ¬åˆ¤æ–­ä¸ºçœŸï¼ŒæŠŠçœŸå®çš„æ ·æœ¬åˆ¤æ–­ä¸ºå‡ï¼Œè¦æœ€å°åŒ–è¿™ä¸¤é¡¹ä¹‹å’Œï¼š$ { E }_{ x ï½ { p }_  { data } (x) }[logD(x)] + { E }_{ z ï½ { p }_{ z }(z) }[log(1-D(G(z)))]\ $
æ€»çš„æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªminmax Gameï¼›

**Note:å®é™…è®­ç»ƒå½“ä¸­ï¼Œè®­ç»ƒ$G$çš„æ—¶å€™$D$çš„å‚æ•°æ˜¯å›ºå®šçš„ï¼Œ$G$å¹¶ä¸å¹²æ‰°$D$å¯¹çœŸå®æ•°æ®çš„åˆ¤æ–­ï¼Œ$G$éœ€è¦$D$çš„æ­£ç¡®å¼•å¯¼ï¼Œ$G$åªæ˜¯ä¸æ–­æå‡è‡ªå·±ç”Ÿæˆæ•°æ®çš„èƒ½åŠ›ã€‚**

-------

## Loss Function

$D$çš„æŸå¤±å‡½æ•°ï¼š
$$Loss_D = -[1*logD(x) + 1*log(1-D(G(z)))] $$
$G$çš„æŸå¤±å‡½æ•°ï¼š
$$Loss_G = 0*logD(x) + 1*log(1-D(G(z)))=log(1-D(G(z)))$$

-------

## å…·ä½“ç®—æ³•è¿‡ç¨‹

![](./res/GAN.jpg)

Noteï¼š
1ã€å…ˆæ›´æ–°$D$ï¼Œå†æ›´æ–°$G$ï¼Œåªæœ‰$D$æœ‰äº†æ­£ç¡®çš„åˆ¤æ–­èƒ½åŠ›ï¼Œ$G$æ‰èƒ½æŒ‰ç…§$D$çš„æŒ‡ç¤ºæ¥æ›´æ–°;
2ã€å¯ä»¥è®¾ç½®ä¸€ä¸ªè¶…å‚æ•°kæ¥åè°ƒ$D$ã€$G$ä¸¤è€…ä¹‹é—´æ›´æ–°çš„æ¬¡æ•°æ¯”ä¾‹;
3ã€åœ¨è®­ç»ƒ$G$çš„æ—¶å€™$D$çš„å‚æ•°è¦å›ºå®šï¼Œåœ¨è®­ç»ƒ$D$çš„æ—¶å€™$G$çš„å‚æ•°è¦å›ºå®š;

-------

## Gæ›¿ä»£ç‰ˆçš„Loss Function

ç”±äº$G(z)$æ˜¯ä»å™ªå£°ä¸­ç”Ÿæˆçš„æ ·æœ¬ï¼Œæ‰€ä»¥åœ¨æœ€å¼€å§‹$G$ç”Ÿæˆçš„æ ·æœ¬éå¸¸å‡ï¼Œå¾ˆå®¹æ˜“è¢«$D$æŠ“å‡ºæ¥ï¼Œä¹Ÿå°±æ˜¯è¯´$D(G(z))$éå¸¸å°,é‚£ä¹ˆ$Loss_G = log(1-D(G(z)))$å°±éå¸¸æ¥è¿‘0ï¼Œåœ¨åå‘ä¼ æ’­çš„æ—¶å€™å°±ä¸èƒ½å¤Ÿä¼ æ’­è¶³å¤Ÿçš„æ¢¯åº¦ç»™$G$æ¥æ›´æ–°å‚æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬ä»Heuristicçš„è§’åº¦æ¥ç†è§£ï¼šæˆ‘ä»¬æœ¬èº«æ˜¯è¦æœ€å°åŒ–$D$æŠ“å‡ºæ¥å‡æ ·æœ¬çš„æ¦‚ç‡ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥æ¢æˆæœ€å¤§åŒ–$D$æŠ“ä¸å‡ºæ¥çš„æ¦‚ç‡ï¼Œä¹Ÿå°±æ˜¯å°†$G$çš„æŸå¤±å‡½æ•°æ¢æˆï¼š
$$Loss_G=-logD(G(z)) $$
ç”±äº$D$æ˜¯æŒ‰ç…§ï¼š
$$Loss_G = log(1-D(G(z)))$$
è®­ç»ƒçš„ï¼Œé‚£ä¹ˆå¦‚æœæŸå¤±å‡½æ•°æ¢æˆï¼š
$$Loss_G=-logD(G(z)) $$
è¿™ä¸¤é¡¹ä¸æ˜¯ç­‰ä»·çš„ï¼Œæ‰€ä»¥$D$ç»™å‡ºçš„å€¼å°±èƒ½å¤Ÿæä¾›è¶³å¤Ÿçš„æ¢¯åº¦ã€‚

**Note:
$Loss_G =log(1-D(G(z)))$å¯¹åº”çš„GANå«åšMMGAN
$Loss_G=-logD(G(z)) $å¯¹åº”çš„GANå«åšNSGAN**

ä»å‡½æ•°å›¾åƒä¸Šï¼Œå¯ä»¥ç›´è§‚çš„çœ‹å‡ºï¼Œä¸¤ç§æŸå¤±å‡½æ•°çš„æ¢¯åº¦å˜åŒ–è¶‹åŠ¿ï¼š
![](./res/æŸå¤±å‡½æ•°å›¾åƒ.jpg)

-------

## Dçš„æœ€ä¼˜è§£

å¯¹äºä¸€ä¸ªå…·ä½“çš„æ ·æœ¬$x$ï¼Œå®ƒå¯èƒ½æ¥è‡ªçœŸå®åˆ†å¸ƒä¹Ÿå¯èƒ½æ¥è‡ªç”Ÿæˆåˆ†å¸ƒï¼Œåˆ¤åˆ«å™¨åˆ¤æ–­çš„ç»“æœæ˜¯$D(x)$,ä»£å…¥$D$çš„æŸå¤±å‡½æ•°æœ‰:
$$Loss_D = -[{P}_{r}(x)*log[D(x)] + {P}_{g}(x)*log[1-D(x)]]$$
å¯¹$D(x)$æ±‚åå¯¼æ•°æœ‰ï¼š
$${ \frac { \partial Loss_{ D } }{ \partial D(x) } =-\frac { { P }_{ r }(x) }{ D(x) }  }+\frac { { P }_{ g }(x) }{ 1-D(x) } $$
ä»¤åå¯¼æ•°ç­‰äº0:
$${ -\frac { { P }_{ r }(x) }{ D(x) }  }+\frac { { P }_{ g }(x) }{ 1-D(x) } =0$$
æ±‚å‡ºæœ€ä¼˜åˆ¤åˆ«å™¨${ D }^{ * }(x)$:
$${ D }^{ * }(x)=\frac { { P }_{ r }(x) }{ { P }_{ r }(x)+{ P }_{g}(x)}$$
æˆ‘ä»¬çŸ¥é“å¯¹äº$G$æ¥è¯´ï¼Œæœ€å¥½çš„$G$æ˜¯è®©ï¼š
$${ P }_{ r }(x) = { P }_{ g }(x)$$
æ­¤æ—¶ï¼Œæœ‰ï¼š
$${ D }^{ * }(x)=1/2$$
ä¹Ÿå°±æ˜¯è¯´æœ€å¥½çš„ç”Ÿæˆå™¨ä½¿æœ€å¥½çš„åˆ¤åˆ«å™¨æ— æ³•åˆ¤åˆ«å‡ºæ¥æ ·æœ¬æ˜¯ç”Ÿæˆæ ·æœ¬è¿˜æ˜¯çœŸå®æ ·æœ¬ã€‚

-------

## <span id="3">å½“Dæœ€ä¼˜æ—¶ï¼ŒGçš„æŸå¤±å‡½æ•°åˆ°åº•åœ¨å¹²ä»€ä¹ˆï¼Ÿ</span>

å°†æœ€ä¼˜åˆ¤åˆ«å™¨${ D }^{ * }(x)$ä»£å…¥Objective Function,æœ‰ï¼š
$$ \min _{G}\max _{ D } V(D,G)={ E }_{ x ï½ { p }_  { data } (x) }log\frac { { P }_{ r }(x) }{ { P }_{ r }(x)+{ P }_{g}(x)} + { E }_{ z ï½ { p }_{ z }(z) }log\frac { { P }_{ g }(x) }{ { P }_{ r }(x)+{ P }_{g}(x)} $$
ç¨ä½œå˜æ¢æœ‰ï¼š
$$ \min _{ G } \max _{ D } V(D,G)={ E }_{ xï½{ p }_{ { data } }(x) }log\frac { { P }_{ r }(x) }{ \frac { 1 }{ 2 } [{ P }_{ r }(x)+{ P }_{ g }(x)] } +{ E }_{ zï½{ p }_{ z }(z) }log\frac { { P }_{ g }(x) }{ \frac { 1 }{ 2 } [{ P }_{ r }(x)+{ P }_{ g }(x)] } -2log2$$

äºæ˜¯ï¼š
$$ \min _{G}\max _{ D } V(D,G)=2JS({ P }_{ r }||{ P }_{ g })-2log2 $$
æ‰€ä»¥ï¼Œå½“åˆ¤åˆ«å™¨$D$æœ€ä¼˜çš„æ—¶å€™ï¼Œç”Ÿæˆå™¨$G$æ˜¯åœ¨å‡å°çœŸå®åˆ†å¸ƒä¸ç”Ÿæˆåˆ†å¸ƒä¹‹é—´çš„$JS$æ•£åº¦ï¼ˆè¿™ä¸ªæ˜¯æ ¹æ®MMGANæ¨å¯¼å¾—åˆ°çš„ç»“è®ºï¼‰ã€‚

**Note:
$KL$æ•£åº¦ï¼š$KL({ P }_{ 1 }||{ P }_{ 2 })={ P }_{ 1 }\log { \frac { { P }_{ 1 } }{ { P }_{ 2 } }  } $
$JS$æ•£åº¦ï¼š$ JS({ P }_{ 1 }||{ P }_{ 2 })=\frac { 1 }{ 2 } KL({ P }_{ 1 }||\frac { { P }_{ 1 }+{ P }_{ 2 } }{ 2 } )+\frac { 1 }{ 2 } KL({ P }_{ 2 }||\frac { { P }_{ 1 }+{ P }_{ 2 } }{ 2 } ) $**

-------

## <span id="4">MMGANçš„é—®é¢˜</span>

å¯¹äºç”Ÿæˆå™¨$G$æ¥è¯´ï¼Œå½“åˆ¤åˆ«å™¨$D$æœ€ä¼˜æ—¶ï¼Œ$G$çš„æŸå¤±å‡½æ•°ä¸ºï¼š
$$Loss_G={ P }_{ g }(x)*log\frac { { P }_{ g }(x) }{ { P }_{ r }(x)+{ P }_{g}(x)}$$
è¿™é‡Œæœ‰ä¸€ä¸ªéš¾ç†è§£çš„ç‚¹ï¼šæˆ‘ä»¬çš„ç›®æ ‡å‡½æ•°æ˜¯$logD(x)$å’Œ$log(1-D(G(z)))$çš„æœŸæœ›ä¹‹å’Œï¼Œåœ¨å®é™…è®­ç»ƒçš„æ—¶å€™æˆ‘ä»¬çŸ¥é“$x$å’Œ$G(z)$åˆ°åº•æ˜¯ç”Ÿæˆçš„è¿˜æ˜¯çœŸå®çš„ï¼Œæ‰€ä»¥${ P }_{ r }$å’Œ${ P }_{ g }$ç›´æ¥æŒ‰ç…§1å’Œ0æ¥ï¼Œä½†æ˜¯æˆ‘ä»¬è€ƒè™‘ä¸€ä¸‹è¿™ç§æƒ…å†µï¼Œå¦‚æœçœŸå®æ•°æ®å’Œç”Ÿæˆæ•°æ®æœ‰ä¸€æ¨¡ä¸€æ ·çš„ï¼Œæˆ‘ä»¬è¿˜èƒ½æŒ‰ç…§1å’Œ0æ¥å—ï¼Ÿå½“ç„¶ä¸æ˜¯ï¼Œè¿™å°±è¦æŒ‰ç…§ä¸¤ç§åˆ†å¸ƒçš„Density Functionæ¥å¾—åˆ°å¯¹äºæ ·æœ¬$x$æ¥è‡ªä¸¤ç§åˆ†å¸ƒçš„æ¦‚ç‡åˆ†åˆ«æ˜¯å¤šå°‘ï¼›æ‰€ä»¥åœ¨è¿™ç§ä¸¤ç±»æ•°æ®æœ‰é‡åˆçš„æƒ…å†µä¸‹${ P }_{ r }$å’Œ${ P }_{ g }$æ˜¯ä¸ç¡®å®šçš„ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆç›®æ ‡å‡½æ•°ç”¨æœŸæœ›æ¥è¡¨è¾¾ï¼›ä½†æ˜¯å®é™…æƒ…å†µä¸­$x$å’Œ$G(z)$é‡åˆçš„æ¦‚ç‡å‡ ä¹ä¸ºé›¶ï¼Œé‚£ä¹ˆå¯¹ä¸$Loss_G$æ¥è¯´ï¼Œ${ P }_{ g }=1$ä¸”${ P }_{ r }=0$ï¼Œ$Loss_G=0$ï¼ŒGæ²¡æœ‰æ¢¯åº¦æ¥æ›´æ–°å‚æ•°ã€‚

ä¸ºä»€ä¹ˆ$x$å’Œ$G(z)$é‡åˆçš„éƒ¨åˆ†å¯ä»¥å¿½ç•¥ä¸è®¡?
* çœŸå®åˆ†å¸ƒä¸ç”Ÿæˆåˆ†å¸ƒçš„æ”¯æ’‘é›†(support)æ˜¯é«˜ç»´ç©ºé—´ä¸­çš„ä½ç»´æµå½¢(manifold)ï¼ŒçœŸå®åˆ†å¸ƒä¸ç”Ÿæˆåˆ†å¸ƒé‡å éƒ¨åˆ†æµ‹åº¦(measure)ä¸º0ï¼Œè¿™å¥è¯å¯ä»¥è¿™ä¹ˆç†è§£ï¼šä»ä¸€ä¸ªä¸‰ç»´ç©ºé—´é‡Œé¢éšæœºå–ä¸¤ä¸ªäºŒç»´æ›²é¢ï¼Œä»–ä»¬é‡åˆçš„éƒ¨åˆ†(çº¿)ç›¸å¯¹äºä»–ä»¬æœ¬èº«(é¢)å¯ä»¥å¿½ç•¥ï¼›åœ¨ä¸€ä¸ªäºŒç»´å¹³é¢ä¸Šéšæœºç”»ä¸¤æ¡æ›²çº¿ä»–ä»¬é‡åˆçš„éƒ¨åˆ†(ç‚¹)ç›¸å¯¹äºä»–ä»¬æœ¬èº«(é¢)å¯ä»¥å¿½ç•¥ã€‚
* é€šä¿—æ¥è®²å°±æ˜¯è¯´éšæ„ä¸€ä¸ªå™ªå£°ï¼Œéšæ„ä¸€ä¸ª$G$ï¼Œèƒ½ç›´æ¥ç”Ÿæˆå‡ºè·ŸçœŸå®åˆ†å¸ƒç›¸åŒçš„æ ·æœ¬æ¥çš„æ¦‚ç‡ä¸º0ï¼Œç”Ÿæˆä¸å‡ºæ¥é‚£ä¹ˆç”Ÿæˆåˆ†å¸ƒå’ŒçœŸå®åˆ†å¸ƒå°±æ²¡æœ‰å¯èƒ½é‡åˆã€‚
* æƒ³è±¡ä¸€ä¸‹å¦‚æœ$G$æ—¢èƒ½ç”Ÿæˆæ¥è¿‘äºçœŸçš„åˆèƒ½ç”Ÿæˆæ¥è¿‘äºå‡çš„ï¼Œé‚£ä¹ˆæŸå¤±å‡½æ•°å°±çŸ¥é“æœå“ªä¸ªæ–¹å‘åšæ¢¯åº¦ä¸‹é™ï¼Œä½†ç”Ÿæˆæ¥è¿‘çœŸå®çš„è¿™ä»¶äº‹å„¿ä¸ä¼šå‘ç”Ÿï¼Œæ‰€ä»¥ç”Ÿæˆå™¨çš„æŸå¤±å‡½æ•°çš„åœ¨ä¹±è·‘ã€‚

**Noteï¼šæœ€ä¼˜çš„åˆ¤åˆ«å™¨åªä¸æ•°æ®æœ¬èº«çš„åˆ†å¸ƒ${ P }_{ r }$å’Œ${ P }_{ g }$æœ‰å…³ï¼Œè€Œ$G(z)$ç»™å‡ºçš„æ•°æ®ä½¿å¾—${ P }_{ r }$å’Œ${ P }_{ g }$ä¸ºå¸¸æ•°ï¼ˆä¸¤è€…é‡å éƒ¨åˆ†å¯å¿½ç•¥ï¼‰ï¼Œç”Ÿæˆå™¨å°†ä¸çŸ¥æ‰€æªã€‚**

-------

## NSGANçš„é—®é¢˜

### åˆ¤åˆ«å™¨æœ€ä¼˜æ—¶NSGANåœ¨åšä»€ä¹ˆï¼Ÿ

* å°†$D^*(x) $ä»£å…¥NSGANçš„$Loss_G$æœ‰ï¼š
$$Loss_G=-{P}_{g}(x)*logD^*(x) \quad (1)$$

* ç”±ä¸Šé¢çš„æ¨å¯¼æœ‰ï¼š
$${P}_{r}(x)*log[D^*(x)] + {P}_{g}(x)*log[1-D^*(x)]=2JS({ P }_{ r }||{ P }_{ g })-2log2  \quad (2)$$

* $KL$æ•£åº¦å˜æ¢
$KL({ P }_{ g }(x)||{ P }_{ r }(x))\\
={ P }_{ g }(x)*\log { \frac { { P }_{ g }(x) }{ { P }_{ r }(x) }  } \\ 
={ P }_{ g }(x)*\log { \frac { { P }_{ g }(x)/({ P }_{ r }(x)+{ P }_{ g }(x)) }{ { P }_{ r }(x)/({ P }_{ r }(x)+{ P }_{ g }(x)) }  } \\ 
={ P }_{ g }(x)*\log  \frac { 1-D^{ * }(x) }{ D^{ * }(x) } \\ 
={ P }_{ g }(x)log[1-D^{ * }(x)]-{ P }_{ g }(x)logD^{ * }(x)  \quad (3)$

ç”±$(1)$ã€$(3)$å¾—ï¼š
$$
Loss_{ G }=KL({ P }_{ g }(x)||{ P }_{ r }(x))-{ P }_{ g }(x)log[1-D^{ * }(x)](4)
$$

ç”±$(2)$ã€$(4)$å¾—ï¼š
$$
Loss_{ G }=KL({ P }_{ g }(x)||{ P }_{ r }(x))-2JS({ P }_{ r }||{ P }_{ g })+{P}_{r}(x)*log[D^*(x)]+2log2
$$

**Noteï¼šä»ä¸Šé¢çš„å¼å­å¯ä»¥çœ‹å‡ºKLæ•£åº¦å’ŒJSæ•£åº¦åŒæ—¶å­˜åœ¨ä¸”æ–¹å‘ç›¸åï¼Œè€ŒJSæ•£åº¦å’ŒKLæ•£åº¦éƒ½æ˜¯è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒè·ç¦»çš„åº¦é‡ï¼Œä¸”æ˜¯å•è°ƒæ€§åŒæ­¥çš„å‡½æ•°ï¼Œè¿™æ ·çš„è¯å°±ä¼šå¯¼è‡´æ¢¯åº¦çš„æ–¹å‘ä¸ç¨³å®šï¼Œä¸€ä¼šå„¿ä¸Šå‡ä¸€ä¼šå„¿ä¸‹é™ï¼Œæ‰€ä»¥è¿™ä¸ªæ›¿ä»£ç‰ˆçš„æŸå¤±å‡½æ•°ä¹Ÿä¸æ˜¯ä¸€ä¸ªå¥½çš„é€‰æ‹©ã€‚**

### <span id="2">åªç”¨KLæ•£åº¦ä¼šæ€æ ·ï¼Ÿ</span>

KL Divergenceä¸æ˜¯å¯¹ç§°çš„è·ç¦»ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼š
$KL({ P }_{ g }(x)||{ P }_{ r }(x)) \neq KL({ P }_{ r }(x)||{ P }_{ g }(x))$

#### ç¬¬ä¸€ç§KLæ•£åº¦

$KL({ P }_{ g }(x)||{ P }_{ r }(x))={ P }_{ g }(x)*\log { \frac { { P }_{ g }(x) }{ { P }_{ r }(x) }  } $

è¾“å…¥ç»™Dçš„ç”Ÿæˆå›¾ç‰‡è¦ä¹ˆé è¿‘çœŸå®åˆ†å¸ƒï¼š
${ P }_{ g }(x)=0 \quad { P }_{ r }(x)=1ï¼›KL({ P }_{ g }(x)||{ P }_{ r }(x))è¶‹è¿‘äº0$ï¼Œå¦‚ä¸‹å›¾ï¼š
![](./res/xlogx.jpg)

è¦ä¹ˆé è¿‘ç”Ÿæˆåˆ†å¸ƒï¼š
${ P }_{ g }(x)=1 \quad { P }_{ r }(x)=0ï¼›KL({ P }_{ g }(x)||{ P }_{ r }(x))è¶‹è¿‘äº+\infty$

æ˜¾ç„¶è¿™ä¸ª$KL$æ•£åº¦æ˜¯æ²¡æœ‰åŠæ³•è®©$G$æ”¶æ•›åˆ°çœŸå®åˆ†å¸ƒçš„
![](./res/KL.jpg)

**Noteï¼šå¦‚ä¸Šå›¾ï¼Œå‡å¦‚è¾“å…¥çš„æ˜¯$x=3$è¿™ä¸ªç‚¹ï¼Œ${ P }_{ g }(x)=0 \quad { P }_{ r }(x)=1$ï¼Œæ­£å¸¸æ¥è¯´ç”Ÿæˆåˆ†å¸ƒåº”è¯¥å»é è¿‘è¿™ä¸ªç‚¹ï¼Œæœ‰è¾ƒè¶³å¤Ÿçš„æ¢¯åº¦ï¼Œè€Œä¸Šè¿°æ•£åº¦å´å¹¶éå¦‚æ­¤ã€‚**

#### <span id="6">ç¬¬äºŒç§KLæ•£åº¦</span>

$KL({ P }_{ r }(x)||{ P }_{ g }(x))={ P }_{ r }(x)*\log { \frac { { P }_{ r }(x) }{ { P }_{ g }(x) }  } $

è¾“å…¥ç»™Dçš„ç”Ÿæˆå›¾ç‰‡è¦ä¹ˆé è¿‘çœŸå®åˆ†å¸ƒï¼š
${ P }_{ g }(x)=0 \quad { P }_{ r }(x)=1ï¼›KL({ P }_{ r }(x)||{ P }_{ g }(x))è¶‹è¿‘äº+\infty$

è¦ä¹ˆé è¿‘ç”Ÿæˆåˆ†å¸ƒï¼š
${ P }_{ g }(x)=1 \quad { P }_{ r }(x)=0ï¼›KL({ P }_{ r }(x)||{ P }_{ g }(x))è¶‹è¿‘äº0$

è¿™ä¸ªæ˜¯æƒ³è¦çš„æ•ˆæœï¼›ä½†æ˜¯è¿™å°±å¯¼è‡´äº†mode missingçš„é—®é¢˜ï¼Œå½“ç”Ÿæˆå™¨ç”Ÿæˆçš„æ ·æœ¬ä¸çœŸå®æ—¶ç”Ÿæˆå™¨çš„æ¢¯åº¦è¾ƒå¤§ï¼Œä¼šåƒçœŸå®åˆ†å¸ƒå¯†åº¦é«˜çš„åŒºåŸŸé è¿‘ï¼Œå½“ç”Ÿæˆå™¨é è¿‘ä¸€ä¸ªçœŸå®åˆ†å¸ƒè¾ƒé«˜çš„åŒºåŸŸå­¦ä¹ ç‡å°±ä¼šè¶‹ç´§äº0ï¼Œè€Œå¦‚æœè¿˜æœ‰å…¶ä»–çš„çœŸå®åˆ†å¸ƒå¯†åº¦è¾ƒé«˜çš„åŒºåŸŸç”Ÿæˆå™¨å°±åˆ°è¾¾ä¸äº†äº†ï¼Œè¿™å°±æ˜¯'mode missing'ï¼Œå›¾ç¤ºå¦‚ä¸‹ï¼š

![](./res/mode_missing.jpg)

**Noteï¼šå¦‚æœ$x$æ¯”è¾ƒå‡${ P }_{ g }(x)$ä¹Ÿä¸çŸ¥é“æœå“ªé‡ŒåŠ¨æ‰å¥½ï¼Œé‚£å°±å‘†ç€ä¸åŠ¨ä¹Ÿå°±æ˜¯$KL$æ•£åº¦è¶‹è¿‘äº0ï¼Œå¦‚æœ$x$æ¯”è¾ƒçœŸ${ P }_{ g }(x)$å°±å¾€$x$é è¿‘ï¼›ä½†æ˜¯$KL$æ•£åº¦åœ¨è¿™ç§æƒ…å†µä¸‹ä¸º0æ˜¯ä¸å¯¹çš„ï¼Œå› ä¸º$KL$è¡¨å¾çš„æ˜¯ä¸¤ä¸ªåˆ†å¸ƒçš„ç›¸ä¼¼åº¦ï¼Œ0è¡¨ç¤ºç›¸åŒï¼Œä¹Ÿå°±æ˜¯${ P }_{ r }(x)={ P }_{ g }(x)$ï¼Œæ„æ€æ˜¯å½“ä¸¤è€…ç›¸åŒäº†å°±ä¸åŠ¨äº†ï¼›è¿™ä¸€ç‚¹åœ¨[One-sided label smoothing] (#1)æå‡ºäº†ä¸€ä¸ªè§£å†³åŠæ³•ã€‚**

-------

## è¡¥å……çŸ¥è¯†

### ä¿¡æ¯é‡

$ I(x) = -\log {p(x)} = \log { \frac { 1}{ p (x) }  } $
ä¸€ä¸ªäº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡è¶Šå¤§ï¼Œè¿™ä»¶äº‹æƒ…å‘ç”Ÿæ‰€åŒ…å«çš„ä¿¡æ¯é‡å°±è¶Šå°ï¼Œæ¯”å¦‚è¯´ä¸€ä¸ªé«˜å¯Œå¸…è¿½æ±‚ä¸€ä¸ªç™½å¯Œç¾ï¼Œè¿½åˆ°æ‰‹äº†æ²¡æœ‰ä»€ä¹ˆç¨€å¥‡çš„ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä»¶æ¦‚ç‡å¾ˆé«˜çš„äº‹æƒ…ï¼Œä½†æ˜¯å¦‚æœä¸€ä¸ªçŸ®ç©·çŸ¬è¿½æ±‚ä¸€ä¸ªç™½å¯Œç¾ï¼Œè¿½åˆ°æ‰‹äº†ï¼Œè¿™ç§äº‹æƒ…å‘ç”Ÿçš„æ¦‚ç‡å¾ˆä½ï¼Œå…¶ä¸­ä¸€å®šæœ‰å…¶ä»–çš„åŸå› ï¼šæ¯”å¦‚è¿™ä¸ªçŸ®ç©·çŸ¬æ•‘è¿‡ç™½å¯Œç¾çš„å‘½æˆ–è€…è¿™ä¸ªçŸ®ç©·çŸ¬å™¨å¤§æ´»å¥½ä¸é»äººï¼Œæ‰€ä»¥æ¦‚ç‡ä½çš„äº‹æƒ…å‘ç”Ÿæ‰€åŒ…å«çš„ä¿¡æ¯é‡å¤§ï¼›ä¸¤ä¸ªç›¸äº’ç‹¬ç«‹çš„äº‹æƒ…åŒæ—¶å‘ç”Ÿçš„ä¿¡æ¯é‡æ˜¯ä¸¤è€…å•ç‹¬å‘ç”Ÿçš„ä¿¡æ¯é‡ä¹‹å’Œã€‚

![](./res/ä¿¡æ¯é‡.jpg)

### ä¿¡æ¯ç†µ

ä¿¡æ¯é‡çš„å‡å€¼
$$H(x) = - \sum _{ x } p(x)log p(x) $$

### Huffman Coding

text = abbcdbcccdccaecfeccc
å¯¹textä¸­çš„å­—æ¯è¿›è¡Œç¼–ç ï¼Œæ€»å…±æœ‰6ä¸ªå­—æ¯ï¼Œå¦‚æœæ¯ä¸ªå­—æ¯çš„ç¼–ç é•¿åº¦ç›¸åŒçš„è¯æœ€å°‘éœ€è¦3ä¸ªå­—èŠ‚ï¼Œå¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š

| Letter  | Frequency | Probability | Bits |
| --- | --- | --- | --- |
| a | 2 | 0.10 | 3*2=6 |
| b | 3 | 0.15 | 3*3=9 |
| c | 10 | 0.50 | 3*10=30 |
| d | 2 | 0.10 | 3*2=6 |
| e | 2 | 0.10 | 3*2=6 |
| f | 1 | 0.05 | 3*1=3 |
| total bits|  | | 60 |

Huffman Codingï¼Œé¦–å…ˆå°†å­—æ¯æŒ‰ç…§ä½¿ç”¨é¢‘ç‡ä»ä½åˆ°é«˜æ’åºï¼šf-a-d-e-b-c

ç”»å‡ºéœå¤«æ›¼æ ‘ï¼š
 ![](./res/Huffmann Coding Tree.jpg)

| Letter  | Frequency | Probability | Bits(Huffmann Coding) |
| --- | --- | --- | --- |
| a | 2 | 0.10 | 5*2=10 |
| b | 3 | 0.15 | 2*3=6 |
| c | 10 | 0.50 | 1*10=10 |
| d | 2 | 0.10 | 4*2=8 |
| e | 2 | 0.10 | 3*2=6 |
| f | 1 | 0.05 | 5*1=5 |
| total bits|  | | 45 |
| average bits|  | | 8.55 |

[Reference](https://www.youtube.com/watch?v=voHOxYyFhlc)

### å¹³å‡ç¼–ç é•¿åº¦

å¯¹æ•°çš„åº•æ•°è®¾ç½®ä¸º2çš„è¯ï¼Œç†µ$H(x) = - \sum _{ x } p(x)log p(x) $å°±æ˜¯å¹³å‡ç¼–ç é•¿åº¦ã€‚

é€šè¿‡ç†µçš„å…¬å¼è®¡ç®—textçš„å¹³å‡ç¼–ç é•¿åº¦ï¼š

```
>>> from math import *
>>> -(2*0.1*log(0.1, 2) + 3*0.15*log(0.15, 2) + 10*0.5*log(0.5, 2) \
... + 2*0.1*log(0.1, 2) + 2*0.1*log(0.1, 2) + 1*0.05*log(0.05, 2))
8.440887779051577
>>> 
```

[Reference](https://blog.csdn.net/AckClinkz/article/details/78740427)

### äº¤å‰ç†µ

$$H(P, Q) = - \sum _{ x } p(x)log q(x) $$
ç”¨ä¼°è®¡ç¼–ç $q(x)$è¿‘ä¼¼çœŸå®ç¼–ç $p(x)$éœ€è¦çš„å¹³å‡ç¼–ç é•¿åº¦

### äº¤å‰ç†µä¸MSE

äº¤å‰ç†µä»£ä»·å‡½æ•°åœ¨ç›¸åŒæ¡ä»¶ä¸‹çš„å­¦ä¹ é€Ÿç‡è¾ƒ$MSE$å¿«ï¼Œåœ¨æ¢¯åº¦åå‘ä¼ æ’­æ—¶ï¼Œä¼šæœ‰ä¸€ä¸ªä¹˜å­ï¼Œè€Œäº¤å‰ç†µä»£ä»·å‡½æ•°èƒ½å¤ŸæŠŠè¿™ä¸ªä¹˜å­ç»™çº¦æ‰ï¼Œå¾ˆå¥½çš„é¿å…äº†å­¦ä¹ é€Ÿåº¦ä¸‹é™ã€‚

### KLæ•£åº¦

åˆåç›¸å¯¹ç†µï¼š$$D_{KL}(P||Q)=- \sum _{ x } p(x)log q(x) + \sum _{ x } p(x)log p(x) =H(P, Q)-H(P)$$

### JSæ•£åº¦

$$D_{JS}(P||Q)={\frac{1}{2}} KL(P||M) + {\frac{1}{2}} KL(Q||M) \quad \quad M = {\frac{1}{2}}(P+Q)$$

-------

## å®éªŒ

[ä»£ç é“¾æ¥](https://github.com/andyhujinzhao/GAN)

-------

## è®ºæ–‡ç ”è¯»

### ç¬¬ä¸€å¤„

**Introduction**
These striking successes have primarily been based on the backpropagation and dropout algorithms, using piecewise linear units [19, 9, 10] which have a particularly well-behaved gradient . Deep generative models have had less of an impact, due to the difï¬culty of approximating many intractable probabilistic computations that arise in maximum likelihood estimation and related strategies, and due to difï¬culty of leveraging the beneï¬ts of **piecewise linear units** in the generative context. We propose a new generative model estimation procedure that sidesteps these difï¬culties.
åœ¨åˆ†ç±»é—®é¢˜ä¸­piecewise linear unitsç»™åä¼ æ’­ç®—æ³•æä¾›äº†å¾ˆå¥½çš„æ¢¯åº¦ï¼Œä½†åœ¨ç”Ÿæˆæ¨¡å‹ä¸­å´ä¸èƒ½å—ç›Šäºpiecewise linear units

### ç¬¬äºŒå¤„

**Related work**
æ¶‰åŠçš„å…¶ä»–ç”Ÿæˆç®—æ³•æœ‰ï¼š
RBMsã€DBMsã€MCMCã€DBNsã€NCE

### ç¬¬ä¸‰å¤„

**Adversarial nets**
In the next section, we present a theoretical analysis of adversarial nets, essentially showing that the training criterion allows one to recover the data generating distribution as G and D are given enough capacity
å¯¹$G$å’Œ$D$æœ‰ä¸€ä¸ªå‡è®¾ï¼šenough capacity

### ç¬¬å››å¤„

**Adversarial nets**
In practice, we must implement the game using an iterative, numerical approach. Optimizing D to completion in the inner loop of training is computationally prohibitive, and on ï¬nite datasets would result in overï¬tting. Instead, we alternate between k steps of optimizing D and one step of optimizing G.
åœ¨å®é™…è®­ç»ƒå½“ä¸­$D$ä¸èƒ½ç›´æ¥è®­ç»ƒå¥½ï¼Œè€Œæ˜¯è®­ç»ƒkæ¬¡$D$åè®­ç»ƒä¸€æ¬¡$G$

### ç¬¬äº”å¤„

**Adversarial nets**
In practice, equation 1 may not provide sufï¬cient gradient for G to learn well. Early in learning, when G is poor, D can reject samples with high conï¬dence because they are clearly different from the training data. In this case, log(1 âˆ’ D(G(z))) saturates. Rather than training G to minimize log(1 âˆ’ D(G(z))) we can train G to maximize log D(G(z)). This objective function results in the same ï¬xed point of the dynamics of G and D but provides much stronger gradients early in learning.
åœ¨æœ€å¼€å§‹è®­ç»ƒçš„æ—¶å€™$G$æ¯”è¾ƒå·®ï¼Œ$D$å¾ˆå®¹æ˜“å°±åˆ¤æ–­å‡ºç”Ÿæˆæ ·æœ¬ï¼Œ$ log(1 âˆ’ D(G(z)))$å°±å¤„åœ¨é¥±å’ŒåŒºï¼Œå¯ä»¥å°†$G$çš„æŸå¤±å‡½æ•°æ›¿æ¢ä¸º$-log D(G(z))$

### ç¬¬å…­å¤„

**Adversarial nets**

![](./res/è®­ç»ƒè¿‡ç¨‹ç¤ºæ„å›¾.jpg)

-------

## GANçš„ä¸è¶³

1ã€ç¥ç»ç½‘ç»œå›ºæœ‰çš„é—®é¢˜ï¼šæ¢¯åº¦æ¶ˆå¤±
2ã€Mode missingé—®é¢˜
3ã€JSæ•£åº¦æˆ–KLæ•£åº¦ä½œä¸ºæŸå¤±å‡½æ•°å¸¦æ¥çš„é—®é¢˜

-------

# ã€ŠImproved Techniques for Training GANsã€‹

[è®ºæ–‡é“¾æ¥](https://arxiv.org/abs/1606.03498)

-------

## è®ºæ–‡ç ”è¯»

### ç¬¬ä¸€å¤„

**Abstract**
We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans ï¬nd visually realistic.
åŠç›‘ç£å­¦ä¹ å’Œå›¾åƒç”Ÿæˆ

### ç¬¬äºŒå¤„

**Abstract**
However, training GANs requires ï¬nding a Nash equilibrium of a non-convex game with continuous, highdimensional parameters. GANs are typically trained using gradient descent techniques that are designed to ï¬nd a low value of a cost function, rather than to ï¬nd the Nash equilibrium of a game. When used to seek for a Nash equilibrium, these algorithms may fail to converge.
æŸå¤±å‡½æ•°æ˜¯éå‡¸çš„ï¼Œæ‰€ä»¥è¦æ‰¾ä¸€ä¸ªçº³ä»€å‡è¡¡çš„æœ€å°å€¼å¾ˆéš¾ã€‚

### ç¬¬ä¸‰å¤„

**Toward Convergent GAN Training**
Unfortunately, a modiï¬cation to $Î¸(D)$ that reduces $J(D)$ can increase $J(G)$ , and a modiï¬cation to $Î¸(G)$ that reduces $J(G)$ can increase $J(D)$ . Gradient descent thus fails to converge for many games.
$D$å‚æ•°çš„æ”¹å˜å¯ä»¥å‡å°$D$çš„æŸå¤±ï¼Œä½†æ˜¯å¯èƒ½å¢åŠ $G$çš„æŸå¤±ï¼Œ$G$å‚æ•°çš„æ”¹å˜å¯ä»¥å‡å°‘$G$çš„æŸå¤±ä½†æ˜¯å¯èƒ½å¢åŠ $D$çš„æŸå¤±ï¼Œå¯¼è‡´å¾ˆå¤šè®­ç»ƒä¸èƒ½æ”¶æ•›ã€‚

### ç¬¬å››å¤„

**Toward Convergent GAN Training**

#### Feature matching

Speciï¬cally, we train the generator to match the expected value of the features on an intermediate layer of the discriminator. This is a natural choice of statistics for the generator to match, since by training the discriminator we ask it to ï¬nd those features that are most discriminative of real data versus data generated by the current model.è®©$G$ç›´æ¥å­¦ä¹ $D$ä¸­é—´å±‚çš„ç‰¹å¾ï¼Œå› ä¸º$D$æ¸…æ¥šå“ªäº›ç‰¹å¾æ˜¯é‡è¦çš„å¯ä»¥å®¹æ˜“åŒºåˆ†å‡ºç”Ÿæˆæ ·æœ¬å’ŒçœŸå®æ ·æœ¬ï¼Œ$G$å°±ç›´æ¥å»å­¦ä¹ è¿™äº›ç‰¹å¾ã€‚ä¸­é—´å±‚çš„æŸå¤±å‡½æ•°è®¾ç½®ä¸ºï¼š$||E_{xâˆ¼p_{data}} f(x) âˆ’ E_{zâˆ¼p_z (z)} f(G(z))||^2 _2 \quad $($f(x) $ denote activations on an intermediate layer of the discriminator)

#### Historical averaging

When applying this technique, we modify each playerâ€™s cost to include a term $||Î¸ âˆ’ \frac {1} {t} \sum_{i=1} ^t  Î¸[i]||^2$ where $Î¸[i]$ is the value of the parameters at past time $i$. The historical average of the parameters can be updated in an online fashion(åœ¨çº¿æ›´æ–°) so this learning rule scales well to long time series. This approach is loosely inspired by the ï¬ctitious play algorithm that can ï¬nd equilibria in other kinds of games. We found that our approach was able to ï¬nd equilibria of low-dimensional, continuous non-convex games, such as the minimax game with one player controlling $x$, the other player controlling $y$, and value function $(f(x) âˆ’ 1)(y âˆ’ 1)$, where $f(x) = x$ for $x < 0$ and $f(x) = x^2$ otherwise,For these same toy games, gradient descent fails by going into extended orbits that do not approach the equilibrium point.å¦‚ä¸Šé¢ç¬¬ä¸‰å¤„æ‰€è¿°ï¼Œåˆ¤åˆ«å™¨å’Œç”Ÿæˆå™¨éƒ½å¤„åœ¨åŠ¨æ€å˜åŒ–ä¸­ï¼Œä¸Šä¸€æ­¥ä¼˜åŒ–çš„ç›®æ ‡ï¼Œä¸‹ä¸€æ­¥ç”±äºå¯¹æ‰‹çš„å˜åŒ–å¯¼è‡´ä¼˜åŒ–ç›®æ ‡éƒ½å˜äº†ï¼Œæ‰€ä»¥å¯ä»¥è€ƒè™‘ç”¨åˆ°å†å²çš„å‚æ•°ä¿¡æ¯æ¥ä¿è¯å‚æ•°åœ¨æ—¶é—´ä¸Šçš„è¿ç»­æ€§ã€‚

**Noteï¼šFictitious play algorithm in game theory, fictitious play is a learning rule first introduced by George W. Brown. In it, each player presumes that the opponents are playing stationary (possibly mixed) strategies. At each round, each player thus best responds to the empirical frequency of play of their opponent. Such a method is of course adequate if the opponent indeed uses a stationary strategy, while it is flawed if the opponent's strategy is non-stationary. The opponent's strategy may for example be conditioned on the fictitious player's last move.**

#### Minibatch discrimination

One of the main failure modes for GAN is for the generator to collapse to a parameter setting where it always emits the same point, the discriminator processes each example independently, there is no coordination between its gradients, and thus no mechanism to tell the outputs of the generator to become more dissimilar to each other. any discriminator model that looks at multiple examples in combination, rather than in isolation, could potentially help avoid collapse of the generator.é€šè¿‡è®©Dä¸€æ¬¡ç»¼åˆæ€§çš„åˆ¤æ–­ä¸€æ‰¹æ ·æœ¬ï¼Œè€Œä¸æ˜¯å•ç‹¬çœ‹å¾…ä¸€æ‰¹æ ·æœ¬ä¸­çš„æ¯ä¸€ä¸ªæ¥é¿å…mode collapseï¼Œå…·ä½“çš„åšæ³•ï¼šè®¾$f(x_i)âˆˆâ„^A$æ˜¯è¾“å…¥$x_i$åœ¨åˆ¤åˆ«å™¨ä¸­é—´å±‚çš„ç‰¹å¾å‘é‡ï¼Œæˆ‘ä»¬å°†$f(x_i)âˆˆâ„^A$ä¹˜ä»¥ä¸€ä¸ªå¼ é‡$Tâˆˆâ„^{A*B*C}$ï¼Œå¾—åˆ°ä¸€ä¸ªçŸ©é˜µ$M_iâˆˆâ„^{B*C}$ï¼Œç„¶åå†æŒ‰è¡Œå¯¹$M_i$ä¸¤ä¸¤è®¡ç®—$L_1$-distanceï¼Œ$i âˆˆ {1, 2, . . . , n}$ï¼Œå†åšå¤„ç†ï¼š$c_b (x_i , x_j ) = exp(âˆ’||M_{i,b} âˆ’ M_{j,b} ||_{L_1} )âˆˆ â„$ï¼Œæ•´ä¸ªè¿‡ç¨‹å›¾ç¤ºå¦‚ä¸‹ï¼š
![](./res/Xnip2018-11-26_17-20-02.jpg)
![](./res/Xnip2018-11-26_17-21-11.jpg)
å†å°†concatenateåçš„ç»“æœfeedç»™ä¸‹ä¸€å±‚ï¼Œæœ€åçš„æŸå¤±å‡½æ•°ä¸€åˆ‡ç…§å¸¸ï¼Œä¸åšå˜åŒ–ï¼Œè¿™æ—¶Då°±able to use the other examples in the minibatch as side information.

#### <span id="1">One-sided label smoothing</span>

replaces the 0 and 1 targets for a classiï¬er with smoothed values, like .9 or .1, and was recently shown to reduce the vulnerability of neural networks to adversarial examples. Replacing positive classiï¬cation targets with Î± and negative targets with Î², the optimal discriminator becomes $D(x) = \frac {Î±p_{data} (x)+Î²p_{ model } (x)}{p_{data} (x)+p_{ model } (x)}$;æŠŠä¼˜åŒ–ç›®æ ‡ä»é™ä½åˆ°$Î±$ï¼ŒThe presence of $p_{model}$ in the numerator(åˆ†å­) is problematic(æœ‰é—®é¢˜çš„) because, in areas where $p_{data}$ is approximately zero and $p_{model}$ is large, erroneous samples from $p_{model}$ have no incentive to move nearer to the data. We therefore smooth only the positive labels to Î±, leaving negative labels set to 0.$D(x) $ä¸­åˆ†å­é¡¹å‡ºç°$p_{model}$çš„é—®é¢˜æ˜¯åœ¨$p_{data}$ä½çš„åŒºåŸŸ$p_{model}$å¯èƒ½å¾ˆé«˜ï¼Œè¿™æ ·çš„è¯$D(x)$å°±å¾ˆå¤§ï¼Œä¸èƒ½ç»™ç”Ÿæˆå™¨incentiveå»é è¿‘$p_{data}$ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æŠŠæ­£æ ·æœ¬çš„æ ‡ç­¾æ¦‚ç‡è°ƒæ•´åˆ°$Î±$ï¼Œè´Ÿæ ·æœ¬çš„æ ‡ç­¾æ¦‚ç‡è®¾ç½®ä¸º0ã€‚

#### Virtual batch normalization

Batch normalization greatly improves optimization of neural networks, and was shown to be highly effective for DCGANs [3]. However, it causes the output of a neural network for an input example $x$ to be highly dependent on several other inputs $x'$ in the same minibatch. To avoid this problem we introduce virtual batch normalization (VBN), **in which each example $x$ is normalized based on the statistics collected on a reference batch of examples that are chosen once and ï¬xed at the start of training, and on $x$ itself.** The reference batch is normalized using only its own statistics. VBN is computationally expensive because it requires running forward propagation on two minibatches of data, so we use it only in the generator network.
VBN:è®­ç»ƒä¹‹åˆï¼Œå…ˆæŒ‰ç…§statisticsé€‰å‡ºæ¥ä¸€ä¸ªreference batchï¼Œç®—å‡ºæ¥å‡å€¼å’Œæ–¹å·®ç”¨äºå¯¹æ¯ä¸ª$x$åšbatch normalizationã€‚

### ç¬¬äº”å¤„

**Assessment of image quality**

#### æ–¹æ³•ä¸€

Generative adversarial networks lack an objective function, which makes it difï¬cult to compare performance of different models. One intuitive metric of performance can be obtained by having human annotators judge the visual quality of samples.
çœŸäººå›¾çµæµ‹è¯•

#### æ–¹æ³•äºŒ

We apply the Inception model^1 [19] to every generated image to get the conditional label distribution $p(y|x)$. Images that contain meaningful objects should have a conditional label distribution $p(y|x)$ with low entropy. Moreover, we expect the model to generate varied images, so the marginal $p(y|x = G(z))dz$ should have high entropy. Combining these two requirements, the metric that we propose is: $exp(E_x KL(p(y|x)||p(y)))$, where we exponentiate results so the values are easier to compare.we ï¬nd between the quality reported by human annotators and the Inception score we developed in Section 4, which is explicitly constructed to measure the â€œobjectnessâ€ of a generated image.
å°†ç”Ÿæˆå‡ºæ¥çš„å›¾ç‰‡å–‚ç»™Inception model

[Reference](https://arxiv.org/abs/1512.00567)

-------

## å®éªŒ

[è®ºæ–‡æºç ](https://github.com/openai/improved-gan)

-------

# ã€ŠTOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKSã€‹

[è®ºæ–‡åœ°å€](https://arxiv.org/abs/1701.04862)

-------

## Motivation

è¿™ç¯‡æ–‡ç« ä¸»è¦ä»åŸç†ä¸Šæ¢è®¨GANå­˜åœ¨çš„é—®é¢˜ï¼Œè€Œä¸æ˜¯è®²ä»»ä½•ä¸€ç§å…·ä½“çš„ç®—æ³•

è¿™ç¯‡paperåˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼š
1ã€æ¢è®¨GANç°æœ‰é—®é¢˜
2ã€ä»åŸç†ä¸Šæ¢ç©¶é—®é¢˜çš„åŸå› 
3ã€æ¢è®¨ä¸€ä¸‹ç°æœ‰çš„å®éªŒè§£å†³æ–¹æ¡ˆå’Œç†è®ºè§£å†³æ–¹æ¡ˆ

-------

## è®ºæ–‡ç ”è¯»

### æ¢è®¨GANç°æœ‰é—®é¢˜

#### KLæ•£åº¦
**INTRODUCTION**
Traditional approaches to generative modeling relied on maximizing likelihood, or equivalently minimizing the Kullback-Leibler (KL) divergence between our unknown data distribution $P_r$ and our generatorâ€™s distribution $P_g$ (that depends of course on $Î¸$). If we assume that both distributions are continuous with densities $P_r$ and $P_g$ , then these methods try to minimize$KL(â„™_r||â„™_g ) =âˆ«_x P_r (x)log {\frac {P_r (x)} {P_g (x)}}d_x$, This cost function has the good property that it has a unique minimum at $P_g = P_r$ , and it doesnâ€™t require knowledge of the unknown $P_r (x) $ to optimize it (only samples). However, it is interesting to see how this divergence is not symetrical between$ P_r$ and $P_g$ :ä¼ ç»Ÿçš„ç”Ÿæˆå™¨ä¾èµ–äºæœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•ç­‰ä»·äºæœ€å°åŒ–çœŸå®åˆ†å¸ƒä¸ç”Ÿæˆåˆ†å¸ƒä¹‹é—´çš„$KL$æ•£åº¦ï¼Œå®ƒçš„ä¼˜ç‚¹åœ¨äºæˆ‘ä»¬å¹¶ä¸éœ€è¦å…·ä½“çš„çŸ¥é“çœŸå®åˆ†å¸ƒï¼Œè€Œåªéœ€è¦å–æ ·å³å¯ï¼Œä½†æ˜¯KLæ•£åº¦æœ‰æ„æ€çš„æ˜¯å®ƒå¹¶ä¸æ˜¯ä¸€ä¸ªå¯¹ç§°çš„è·ç¦»ï¼šè¿™ä¸€ç‚¹åœ¨åŸå§‹GANçš„è®ºæ–‡ç¬”è®°ï¼š[åªç”¨KLæ•£åº¦ä¼šæ€æ ·ï¼Ÿ] (#2)ä¸­å·²ç»åšäº†è¯¦ç»†çš„è§£é‡Šï¼Œåœ¨è¿™é‡Œä»…æ”¾å‡ºæœ¬è®ºæ–‡çš„åŸæ–‡ï¼š
* If $P_r(x) > P_g(x)$, then $x$ is a point with higher probability of coming from the data than being a generated sample. This is the core of the phenomenon commonly described as â€˜mode droppingâ€™: when there are large regions with high values of $P_r$ , but small or zero values in $P_g$ . It is important to note that when $P_r (x) > 0$ but $P_g (x) â†’ 0$, the integrand inside the $KL$ grows quickly to inï¬nity, meaning that this cost function assigns an extremely high cost to a generatorâ€™s distribution not covering parts of the data.
* If $P_r(x) < P_g(x)$, then $x$ has low probability of being a data point, but high probability of being generated by our model. This is the case when we see our generator outputting an image that doesnâ€™t look real. In this case, when $P_r (x) â†’ 0$ and $P_g (x) > 0$, we see that the value inside the $KL$ goes to 0, meaning that this cost function will pay extremely low cost for generating fake looking samples.

#### JSæ•£åº¦
å‚è€ƒï¼š
1ã€[å½“Dæœ€ä¼˜æ—¶ï¼ŒGçš„æŸå¤±å‡½æ•°åˆ°åº•åœ¨å¹²ä»€ä¹ˆï¼Ÿ] (#3)
2ã€[MMGANçš„é—®é¢˜] (#4)

#### è‹¥å¹²é—®é¢˜
* Why do updates get worse as the discriminator gets better? Both in the original and the new cost function.

* Why is GAN training massively unstable?

* Is the new cost function following a similar divergence to the JSD? If so, what are its properties?

* Is there a way to avoid some of these issues?

##### SOURCES OF INSTABILITY
###### discriminator cost is maxed out
The theory tells us that the trained discriminator will have cost at most $2log2 âˆ’ 2JSD(P_r ||P_ g )$. However, in practice, if we just train $D$ till convergence, its error will go to $0$, as observed in Figure 1
![Figure 1](./res/figure_1.jpg)
pointing to the fact that the $JSD$ between them is maxed out. The only way this can happen is if the distributions are not continuous^1 , or they have disjoint supports.
ä»ç†è®ºä¸Šè®²æœ€ä¼˜åˆ¤åˆ«å™¨çš„costä¸º$2log2 âˆ’ 2JSD(P_r ||P_ g )$ï¼Œåœ¨[å½“Dæœ€ä¼˜æ—¶ï¼ŒGçš„æŸå¤±å‡½æ•°åˆ°åº•åœ¨å¹²ä»€ä¹ˆï¼Ÿ] (#3)å¤„æœ‰è¯¦ç»†æ¨å¯¼ï¼Œä½†æ˜¯åœ¨å®é™…è®­ç»ƒä¸­æŠŠ$D$ trainåˆ°æ”¶æ•›è¯¯å·®å°†å˜ä¸º0ï¼Œè¯¯å·®å¯ä»¥è®­ç»ƒåˆ°0è¯´æ˜$JSD(P_r ||P_ g )$å¯ä»¥è®­ç»ƒåˆ°1ï¼Œè€Œ$JSD$æœ‰ç•Œä¸”å…¶å–å€¼åŒºé—´ä¸º[0, 1]ï¼Œå½“${ P }_{ r }(x)=1 \quad { P }_{ g }(x)=0$æˆ–${ P }_{ r }(x)=0 \quad { P }_{ g }(x)=1$æ—¶å–1ï¼Œæœ‰ç•Œæ€§è¯æ˜[ç¨åè§£é‡Š](#5)ï¼›$D$èƒ½ä½¿$JSD$å§‹ç»ˆå–ä¸Šç•Œè¯´æ˜ä¸¤ä¸ªåˆ†å¸ƒå§‹ç»ˆå¤„äºå¯ä»¥å¤„äº$JS$ æ•£åº¦çš„åº¦é‡æ–¹å¼ä¸‹çš„æœ€å¤§é—´è·:${ P }_{ r }(x)=1 \quad { P }_{ g }(x)=0$æˆ–${ P }_{ r }(x)=0 \quad { P }_{ g }(x)=1$ï¼Œé‚£ä¹ˆä»€ä¹ˆæ ·çš„åˆ†å¸ƒèƒ½ä½¿å¾—$P_g$å’Œ$P_r$å§‹ç»ˆå¤„äº$JSD$çš„ä¸Šç•Œï¼Ÿå”¯ä¸€å¯èƒ½çš„è§£é‡Šæ˜¯the distributions are not continuousï¼Œè¿™é‡Œæœ‰ç‚¹æ¯”è¾ƒéš¾ç†è§£ã€‚æ˜¯ä¸‹é¢è¿™æ ·å—ï¼Ÿ
![](./res/not_continuous_1.jpg)
æ˜¾ç„¶ä¸æ˜¯ï¼Œè™½ç„¶æœ‰mode missingçš„é—®é¢˜ï¼Œä½†æ˜¯åªè¦ä»–ä»¬æœ‰é‡åˆçš„éƒ¨åˆ†$JSD$å°±ä¸å¯èƒ½å–ä¸Šç•Œï¼Œæ–‡ä¸­è¯´åˆ°The only way this can happen is if the distributions are not continuousï¼›ä¸Šé¢çš„å›¾ç¤ºä¸æ˜¯not continuousï¼Œæ–‡ä¸­æ³¨é‡Šè¯´ï¼šBy continuous we will actually refer to an absolutely continuous random variable (i.e. one that has a density), as it typically done. For further clariï¬cation see Appendix B.ç¡®åˆ‡æ¥è¯´ï¼Œè§[å…³äºè¿ç»­çš„å®šä¹‰](#7)

There is strong empirical and theoretical evidence to believe that $P_r$ is indeed extremely concentrated on a low dimensional manifold (Narayanan & Mitter, 2010). As of $P_g$, we will prove soon that such is the case as well.
$P_r$éå¸¸é›†ä¸­åœ¨ä¸€ä¸ªä½manifold (Narayanan & Mitter, 2010)ï¼Œ$P_g$ä¹Ÿæ˜¯ï¼Œåœ¨[è¿™é‡Œ](#8)ç»™å‡ºäº†è¯æ˜ã€‚

å¦‚æœå¯¹äº$P_r$æ¥è¯´æ²¡æœ‰densityï¼Œé‚£ä¹ˆ$D$å­¦åˆ°çš„å¹¶ä¸æ˜¯dataçš„densityï¼Œè€Œæ˜¯ä½ç»´æµå½¢è€Œå·²ï¼Œæ‰€ä»¥$G$å­¦ä¹ åˆ°çš„åªæ˜¯ä¸ªåˆ«å¤„çš„ä½ç»´æµå½¢ï¼Œä¸ºä»€ä¹ˆä¸æ˜¯å…¨éƒ¨ä½ç»´æµå½¢ï¼Ÿå› ä¸ºæœ‰mode missingé—®é¢˜çš„å­˜åœ¨ï¼Œmode missingå‚è€ƒ[ç¬¬äºŒç§KLæ•£åº¦] (#6)ï¼Œé‚£ä¹ˆè¿™æ ·çš„è¯$P_r$å’Œ$P_g$çš„åˆ†å¸ƒå‡ ä¹æ²¡æœ‰é‡åˆéƒ¨åˆ†ï¼Œæ‰€ä»¥åªè¦$D$ä¸€ç›´trainä¸‹å»$JSD$å°±èƒ½åˆ°$0$ã€‚
###### <span id = '8'>æ°¸è¿œæœ‰ä¸€ä¸ªå®Œç¾çš„D </span>
Let $g : Z â†’ X$ be a function composed by afï¬ne transformations and pointwise nonlinearities, which can either be rectiï¬ers, leaky rectiï¬ers, or smooth strictly increasing functions (such as the sigmoid, tanh, softplus, etc). Then, $g(Z)$ is contained in a countable union of manifolds of dimension at most dim $Z$. Therefore, if the dimension of $Z$ is less than the one of $X$, $g(Z)$ will be a set of measure 0 in $X$.ç”¨ä¸‹å›¾æ¥è¡¨ç¤ºï¼š
![](./res/z-g(z).jpg)
Driven by this, this section shows that if the supports of $P_r$ and $P_g$ are disjoint or lie in low dimensional manifolds, there is always a perfect discriminator between them, and we explain exactly how and why this leads to an unreliable training of the generator.
$P_r$ and $P_g$ä¸¤ä¸ªéƒ½æ˜¯é«˜ç»´ç©ºé—´çš„disjointä½ç»´æµå½¢ï¼Œæ‰€ä»¥æ€»æœ‰ä¸€ä¸ªå®Œç¾çš„$D$å­˜åœ¨
###### <span id="7">å…³äºè¿ç»­çš„å®šä¹‰</span>
There are two different but very related properties a random variable can have. A random variable $X$ is said to be continuous if $P(X = x) = 0$ for all single points $x âˆˆ X$. Note that a random variable concentrated on a low dimensional manifold such as a plane can have this property. è¿ç»­å‹éšæœºå˜é‡æ¯ä¸ªç‚¹çš„æ¦‚ç‡éƒ½ä¸º0
![](./res/ä¸‰ç»´ç©ºé—´çš„ç‚¹.jpg)
However, an absolutely continuous random variable has the following property: if a set $A$ has Lebesgue measure 0, then $P(X âˆˆ A) = 0$. Since points have measure 0 with the Lebesgue measure, absolute continuity implies continuity. 
å‹’è´æ ¼æµ‹åº¦ï¼šä¸€ä¸ªçº¿æ®µ$[a, b]$çš„å‹’è´æ ¼æµ‹åº¦æ˜¯å®ƒçš„é•¿åº¦$b-a$ï¼Œä¸€ä¸ªé•¿æ–¹å½¢$[a, b]$ï¼Œ$[c, d]$çš„å‹’è´æ ¼æµ‹åº¦ä¸ºå®ƒçš„é¢ç§¯$(b-a)*(d-c)$ï¼Œä¸€ä¸ªç«‹æ–¹ä½“$[a, b]$ï¼Œ$[c, d]$ï¼Œ$[e, f]$çš„å‹’è´æ ¼æµ‹åº¦æ˜¯å®ƒçš„ä½“ç§¯$(b-a)*(d-c)*(f-e)$,ä»¥æ­¤ç±»æ¨ï¼›å¦‚æœä¸€ä¸ªé›†åˆ$A$çš„å‹’è´æ ¼æµ‹åº¦ä¸º0ï¼Œåˆ™$P(X âˆˆ A) = 0$ï¼Œç»å¯¹è¿ç»­æ„å‘³ç€è¿ç»­,å› ä¸ºè¿ç»­æ˜¯é’ˆå¯¹ç‚¹æ¥è¯´çš„ï¼Œæ¡ä»¶æ›´å¼ºï¼›
![](./res/ä¸‰ç»´ç©ºé—´çš„é¢çš„æµ‹åº¦ä¹Ÿä¸º0.jpg)
A random variable thatâ€™s supported on a low dimensional manifold therefore will not be absolutely continuous: let $M$ a low dimensional manifold be the support of $X$. Since a low dimensional manifold has $0$ Lebesgue measure, this would imply $P(X âˆˆ M) = 0$, which is an absurd since $M$ was the support of $X$. The property of $X$ being absolutely continuous can be shown to be equivalent to $X $ having a density: the existence of a function $f$ : $X â†’ R $ such that $P(X âˆˆ A) = \int f(x) dx $ (this is a consequence of the Radon-Nikodym theorem). 
ä¸€ä¸ªéšæœºå˜é‡$X$çš„æ”¯æ’‘é›†$M$æ˜¯é«˜ç»´ç©ºé—´çš„ä½ç»´æµå½¢ï¼Œå¦‚æœè¯´æ˜¯è¿ç»­çš„é‚£ä¹ˆå®ƒçš„æ”¯æ’‘é›†$P(X âˆˆ M) = 0$ï¼Œè¿™å°±å‘µå‘µäº†ï¼Œå› ä¸º$P(X âˆˆ M) = 1$ï¼›

A The annoying part is that in everyday paper writing when we talk about continuous random variables, we omit the â€˜absolutelyâ€™ word to keep the text concise and actually talk about absolutely continuous random variables (ones that have a density), this is done through almost all sciences and throughout mathematics as well, annoying as it is. However we made the clariï¬cation in here since itâ€™s relevant to our paper not to mistake the two terms.
###### <span id="5">å…³äºJSDæœ‰ç•Œä¸”å…¶å–å€¼åŒºé—´ä¸º[0, 1]çš„è¯æ˜</span>
[Referenceï¼šJensen Inequality] (https://en.wikipedia.org/wiki/Jensen%27s_inequality)$âˆ«p(x)f(ğµ(x))dxâ‰¥f(âˆ«ğµ(x)p(x)dx)$
Given $f(ğµ(x))$ is a convex function and $0â‰¤pâ‰¤1$
$p*f(x1)+(1-p)*f(x2)â‰¥f[p*x1+(1-p)*x2]$  æ¨å¹¿ä¸€ä¸‹ï¼Œæœ‰ï¼š
$p_1*f(x_1)+p_2*f(x_2)+...+p_n*f(x_n)â‰¥f[p_1*x_1+p_2*x_2...+p_n*x_n]$
å³ï¼š$âˆ«p(x)f(ğµ(x))dxâ‰¥f(âˆ«ğµ(x)p(x)dx) \quad å…¶ä¸­ï¼šâˆ«p(x)=0$

$2*JSD(P_r ||P_ g ) $
$= âˆ‘_{x} P_r(x)*log\frac { { P }_{ r }(x) }{ \frac { 1 }{ 2 } [{ P }_{ r }(x)+{ P }_{ g }(x)] } +âˆ‘_{x}P_g(x) * log\frac { { P }_{ g }(x) }{ \frac { 1 }{ 2 } [{ P }_{ r }(x)+{ P }_{ g }(x)] } $
$\ge log âˆ‘_{x} \frac { { P }_{ r }^2(x) }{ \frac { 1 }{ 2 } [{ P }_{ r }(x)+{ P }_{ g }(x)] } +âˆ‘_{x}log\frac { { P }_{ g }^2(x) }{ \frac { 1 }{ 2 } [{ P }_{ r }(x)+{ P }_{ g }(x)] }$ 
$= log âˆ‘_{x} {\frac{4{ P }^2_{ r }(x){ P }^2_{ g }(x)}{[{ P }_{ r }(x)+{ P }_{ g }(x)] ^2}}$ 
$\ge logâˆ‘_{x}{\frac{4P^2_r(x)P^2_g(x)}{4P_r(x)P_g(x)}}$å½“ä¸”ä»…å½“${ P }_{ r }(x)={ P }_{ g }(x)$æ—¶å–ç­‰å·
$ = logâˆ‘_{x}{P_r(x)P_g(x)}$
$=log1$
$=0$

$2*JSD(P_r ||P_ g ) $
$= âˆ‘_{x} P_r(x)*log\frac { { P }_{ r }(x) }{ \frac { 1 }{ 2 } [{ P }_{ r }(x)+{ P }_{ g }(x)] } +âˆ‘_{x}P_g(x) * log\frac { { P }_{ g }(x) }{ \frac { 1 }{ 2 } [{ P }_{ r }(x)+{ P }_{ g }(x)] } $
$\le  âˆ‘_{x} P_r(x)*log\frac { { P }_{ r }(x)+{ P }_{g}(x) }{ \frac { 1 }{ 2 } [{ P }_{ r }(x)+{ P }_{ g }(x)] } +âˆ‘_{x}P_g(x) * log\frac { { P }_{ g }(x)+{ P }_{ r }(x) }{ \frac { 1 }{ 2 } [{ P }_{ r }(x)+{ P }_{ g }(x)] } =log^4_2=2$
å½“${ P }_{ r }(x)=1 \quad { P }_{ g }(x)=0$æˆ–${ P }_{ r }(x)=0 \quad { P }_{ g }(x)=1$æ—¶å–ç­‰å·

ç»¼ä¸Šï¼Œ$0\le2*JSD(P_r ||P_ g ) \le 2$ï¼›å³ $0 \le JSD(P_r ||P_ g ) \le 1$
###### æ€»ç»“
Let $P_r$ and $P_g$ be two distributions whose support lies in two manifolds $M$ and $P$ that donâ€™t have full dimension and donâ€™t perfectly align. We further assume that $P_r$ and $P_g$ are continuous in their respective manifolds. Then,
$JSD(P_r||P_g ) = log 2 $
$KL(P_r||P_g ) = +âˆ $
$KL(P_g||P_r ) = +âˆ$
æ‰€ä»¥$JSD$å’Œ$KLD$ä¸æ˜¯å¥½çš„åº¦é‡ä¸¤ä¸ªæ¯«æ— é‡åˆéƒ¨åˆ†çš„åˆ†å¸ƒçš„ç›¸ä¼¼åº¦çš„å¥½æ–¹æ³•
##### TOWARDS SOFTER METRICS AND DISTRIBUTIONS
###### åŠ å™ªå£°
An important question now is how to ï¬x the instability and vanishing gradients issues. Something we can do to break the assumptions of these theorems is add continuous noise to the inputs of the discriminator, therefore smoothening the distribution of the probability mass. If $X$ has distribution $P_X$ with support on $M$ and  $â„‡$  is an absolutely continuous random variable with density $P$  , then $P_ {â„‡+X}$   is absolutely continuous with density:
$P_{X+â„‡}(x)= E_{yâˆ¼P_X} [P_â„‡  (x âˆ’ y)] = \int_M{P_ {â„‡} (x âˆ’ y) dP_{X(y)}}$
è™½ç„¶$X$ä¸æ˜¯absolutely continuousï¼Œä½†$â„‡$æ˜¯absolutely continuousï¼Œ$â„‡+X$å°±æ˜¯absolutely continuousäº†ï¼Œè¿™å°±æ˜¯é€šå¸¸è¯´çš„åŠ éšæœºå™ªå£°çš„æ–¹æ³•ï¼›
###### Wassersteinè·ç¦»
We recall the deï¬nition of the Wasserstein metric W(P, Q) for P and Q two distributions over X. Namely,
![](./res/æ¨åœŸæœºè·ç¦».jpg)
Wassersteinè·ç¦»

-------

#ã€ŠUnsupervised Representation Learning with Deep Convolutional Generative Adversarial Networksã€‹

[è®ºæ–‡åœ°å€](https://arxiv.org/abs/1511.06434)

-------


